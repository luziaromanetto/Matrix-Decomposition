library(datasets)
data(iris)
dataset <- t(matrix(ts(iris[,1:4]),150,4))
result <- nmf(dataset,3)
w <- basis(result)
h <- coef(result)
pal <- rgb(c(27, 217, 117), c(158, 95, 112), c(119, 2, 179), maxColorValue = 255)
plot(w[1,],w[2,])
points(iris[,1],iris[,2], col = pal[iris[, 5]])
maxcoef <- c()
class <-c()
for( i in 1:150){
coefmax <- which.max(h[,i])
maxcoef <- c(maxcoef, coefmax)
class <- c(class, iris[i,5])
}
classes <- matrix(c(class,maxcoef),150,2)
classesMat <- matrix(0,3,3)
rownames(classesMat)  <- c("iris1", "iris2", "iris3")
colnames(classesMat)  <- c("base1", "base2", "base3")
for( i in 1:3){
for( j in 1:3){
classesMat[i,j]=sum(classes[classes[,1]==i,2]==j)
}
}
classesMat
library(datasets)
data(iris)
dataset <- t(matrix(ts(iris[,1:4]),150,4))
result <- nmf(dataset,3)
w <- basis(result)
h <- coef(result)
pal <- rgb(c(27, 217, 117), c(158, 95, 112), c(119, 2, 179), maxColorValue = 255)
plot(w[1,],w[2,])
points(iris[,1],iris[,2], col = pal[iris[, 5]])
maxcoef <- c()
class <-c()
for( i in 1:150){
coefmax <- which.max(h[,i])
maxcoef <- c(maxcoef, coefmax)
class <- c(class, iris[i,5])
}
classes <- matrix(c(class,maxcoef),150,2)
classesMat <- matrix(0,3,3)
rownames(classesMat)  <- c("iris1", "iris2", "iris3")
colnames(classesMat)  <- c("base1", "base2", "base3")
for( i in 1:3){
for( j in 1:3){
classesMat[i,j]=sum(classes[classes[,1]==i,2]==j)
}
}
classesMat
library(datasets)
data(iris)
dataset <- t(matrix(ts(iris[,1:4]),150,4))
result <- nmf(dataset,3)
w <- basis(result)
h <- coef(result)
pal <- rgb(c(27, 217, 117), c(158, 95, 112), c(119, 2, 179), maxColorValue = 255)
plot(w[1,],w[2,])
points(iris[,1],iris[,2], col = pal[iris[, 5]])
maxcoef <- c()
class <-c()
for( i in 1:150){
coefmax <- which.max(h[,i])
maxcoef <- c(maxcoef, coefmax)
class <- c(class, iris[i,5])
}
classes <- matrix(c(class,maxcoef),150,2)
classesMat <- matrix(0,3,3)
rownames(classesMat)  <- c("iris1", "iris2", "iris3")
colnames(classesMat)  <- c("base1", "base2", "base3")
for( i in 1:3){
for( j in 1:3){
classesMat[i,j]=sum(classes[classes[,1]==i,2]==j)
}
}
classesMat
library(datasets)
data(iris)
dataset <- t(matrix(ts(iris[,1:4]),150,4))
result <- nmf(dataset,3,"lee")
w <- basis(result)
h <- coef(result)
pal <- rgb(c(27, 217, 117), c(158, 95, 112), c(119, 2, 179), maxColorValue = 255)
plot(w[1,],w[2,])
points(iris[,1],iris[,2], col = pal[iris[, 5]])
maxcoef <- c()
class <-c()
for( i in 1:150){
coefmax <- which.max(h[,i])
maxcoef <- c(maxcoef, coefmax)
class <- c(class, iris[i,5])
}
classes <- matrix(c(class,maxcoef),150,2)
classesMat <- matrix(0,3,3)
rownames(classesMat)  <- c("iris1", "iris2", "iris3")
colnames(classesMat)  <- c("base1", "base2", "base3")
for( i in 1:3){
for( j in 1:3){
classesMat[i,j]=sum(classes[classes[,1]==i,2]==j)
}
}
classesMat
50+37+34
121/150
require(tm)
#require(Snowball)
require(SnowballC)
# getAllWords("../aulas/datasets/20_newsgroups")
getAllWords <- function(directory) {
classesDir = system(paste("ls ",directory), intern=T)
allDocuments = c()
ret = list()
ret$numberOfDocuments = 0
for (i in 1:length(classesDir)) {
# Lista os documentos do diretório
docList = system( paste("ls ",directory, "/", classesDir[i], sep="")
, intern=T)
# Imprimir andamento do metodo
cat(i, " out of ", length(classesDir), "\n")
# Para cada documento do diretorio
for (j in 1:length(docList)) {
# constroe uma string com o endereço do documento
document = paste( directory, "/", classesDir[i], "/",
docList[j], sep="")
#print(document)
#PlainTextDocument(readLines(document))
# Le o documento e inclui seu conteundo na variavel allDocuments
# que ao final conterá o conteudo de todos os documentos
allDocuments = c(allDocuments,
readLines(document, encoding = "latin1"))
# Conta o numero de arquivos total
ret$numberOfDocuments = ret$numberOfDocuments + 1
}
}
# Objeto da biblioteca Snowball (acho), que classifica o conteudo de lingua inglesa
data = PlainTextDocument(allDocuments, language="en")
# cria um ponteiro de função
strsplit_space_tokenizer <- function(x) unlist(strsplit(x, "[[:space:]]+"))
# vetor de argumentos para o metodo de processamento
ctrl <- list(tokenize = strsplit_space_tokenizer, # cada conjunto de letras separados por espaço sera considerada uma palavra
removePunctuation = list(preserve_intra_word_dashes = TRUE), # remove pontuação
stopwords = stopwords("en"),     # remove palavras insignificantes
stemming = TRUE,                 # passa as palavras para o radical
wordLengths = c(4, Inf))         # Retira palavras com menos de 4 letras
# Faz o processamento que retorna um vetor de frequancias de cada palavra presente em
# pelo menos um documento do dataset
frequency = termFreq(data, control = ctrl)
# lista de palavras presentes nos documentos
ret$vocabulary = names(frequency)
# lista de diretorios no diretorio raiz, que é usado como classes dos documentos
ret$classesDir = classesDir
# retorna o objeto que contem :
#         ret$numberOfDocuments : numero de documentos
#         ret$vocabulary        : palavras com ocorrencia
#         ret$classesDir        : classes de classificação
ret
}
computeProbability <- function(directory, vocabulary, numberOfDocuments) {
classesDir = system(paste("ls ",directory), intern=T)
allDocuments = c()
pwv_classes = NULL
pv = rep(0, length(classesDir))
for (i in 1:length(classesDir)) {
docList = system(paste("ls ",directory, "/", classesDir[i], sep=""), intern=T)
cat(i, " out of ", length(classesDir), "\n")
# computando probabilidade da classe p(v)
pv[i] = length(docList) / numberOfDocuments
joinDocs = c()
# percorrendo a classe
for (j in 1:length(docList)) {
document = paste(directory, "/", classesDir[i], "/", docList[j], sep="")
#PlainTextDocument(readLines(document))
joinDocs = c(joinDocs, readLines(document, encoding = "latin1"))
}
data = PlainTextDocument(joinDocs, language="en")
#data = removePunctuation(data)		# remove punctuation
#data = removeWords(data, stopwords())	# remove stopwords
#data = stripWhitespace(data)		# strip whitespaces
#data = removeNumbers(data)		# remove numbers
#data = stemDocument(data)		# steming
#frequency = termFreq(data)
strsplit_space_tokenizer <- function(x) unlist(strsplit(x, "[[:space:]]+"))
ctrl <- list(tokenize = strsplit_space_tokenizer,
removePunctuation = list(preserve_intra_word_dashes = TRUE),
stopwords = stopwords("en"),
stemming = TRUE,
wordLengths = c(4, Inf))
frequency = termFreq(data, control = ctrl)
n = length(frequency)
termsUnderThisClass = names(frequency)
counting = rep(0, length(vocabulary))
vocabIndex = c()
for (k in 1:length(termsUnderThisClass)) {
vocabIndex = c(vocabIndex, which(termsUnderThisClass[k] == vocabulary))
}
#print(vocabIndex)
counting[vocabIndex] = frequency
pwv_documentClass = (counting + 1) / (n + length(vocabulary))
pwv_classes = rbind(pwv_classes, pwv_documentClass)
}
ret = list()
ret$pv = pv
ret$pwv_classes = pwv_classes
ret
}
queryNaiveBayes <- function(naiveBayesModel, documentFilename) {
queryDoc = PlainTextDocument(readLines(documentFilename, encoding = "latin1"), language="en")
strsplit_space_tokenizer <- function(x) unlist(strsplit(x, "[[:space:]]+"))
ctrl <- list(tokenize = strsplit_space_tokenizer,
removePunctuation = list(preserve_intra_word_dashes = TRUE),
stopwords = stopwords("en"),
stemming = TRUE,
wordLengths = c(4, Inf))
frequency = termFreq(queryDoc, control = ctrl)
#print(naiveBayesModel$probs$pv)
result = naiveBayesModel$probs$pv #rep(0, length(naiveBayesModel$probs$pv))
#print(result)
termNames = names(frequency)
allProbs = naiveBayesModel$probs$pwv_classes
#allProbs = ((allProbs - min(allProbs)) / (max(allProbs) - min(allProbs))) + 1
for (j in 1:length(naiveBayesModel$probs$pv)) {
#result[j] = naiveBayesModel$probs$pv[j]
sum = 1
for (i in 1:length(termNames)) {
#prod = prod * naiveBayesModel$probs$pwv_classes[j,which(termNames[i] == naiveBayesModel$words$vocabulary)]
sum = sum + log(allProbs[j,which(termNames[i] == naiveBayesModel$words$vocabulary)])
#cat(termNames[i], " ", naiveBayesModel$probs$pwv_classes[j,which(termNames[i] == naiveBayesModel$words$vocabulary)], " ", prod, "\n")
}
result[j] = -log(result[j]) - sum
}
#print(result)
ret = list()
ret$relativeProbability = result / sum(result)
ret$classes = naiveBayesModel$words$classesDir
ret
}
naiveBayes <- function(directory) {
words = getAllWords(directory)
probs = computeProbability(directory, words$vocabulary, words$numberOfDocuments)
ret = list()
ret$words = words
ret$probs = probs
ret
}
test <- function(directory, model) {
classesDir = system(paste("ls ",directory), intern=T)
right = 0
wrong = 0
for (i in 1:length(classesDir)) {
docList = system(paste("ls ",directory, "/", classesDir[i], sep=""), intern=T)
cat(i, " out of ", length(classesDir), "\n")
for (j in 1:length(docList)) {
document = paste(directory, "/", classesDir[i], "/", docList[j], sep="")
result = queryNaiveBayes(model, document)
idx = which.min(result$relativeProbability)
#print(result$relativeProbability)
if (i == idx) {
right = right + 1
} else {
wrong = wrong + 1
}
cat("right: ", right, " wrong: ", wrong, "\n")
}
}
ret = list()
ret$right = right
ret$wrong = wrong
ret
}
naiveBayes("Dropbox/Doutorado/disciplinas/SCC5871/trabalho/datasets/20_newsgroups3")
model <- naiveBayes("Dropbox/Doutorado/disciplinas/SCC5871/trabalho/datasets/20_newsgroups3")
model
model$words
model$words[0:300]
model$words[[0:300]]
model$words
model[words]
model[[words]]
model[["words"]]
model[["words"]][0:300]
model[["words"]]
words= model[["words"]]
model$words$vocabulary[0:300]
require(tm)
#require(Snowball)
require(SnowballC)
# getAllWords("../aulas/datasets/20_newsgroups")
getAllWords <- function(directory) {
classesDir = system(paste("ls ",directory), intern=T)
allDocuments = c()
ret = list()
ret$numberOfDocuments = 0
for (i in 1:length(classesDir)) {
# Lista os documentos do diretório
docList = system( paste("ls ",directory, "/", classesDir[i], sep="")
, intern=T)
# Imprimir andamento do metodo
cat(i, " out of ", length(classesDir), "\n")
# Para cada documento do diretorio
for (j in 1:length(docList)) {
# constroe uma string com o endereço do documento
document = paste( directory, "/", classesDir[i], "/",
docList[j], sep="")
#print(document)
#PlainTextDocument(readLines(document))
# Le o documento e inclui seu conteundo na variavel allDocuments
# que ao final conterá o conteudo de todos os documentos
allDocuments = c(allDocuments,
readLines(document, encoding = "latin1"))
# Conta o numero de arquivos total
ret$numberOfDocuments = ret$numberOfDocuments + 1
}
}
# Objeto da biblioteca Snowball (acho), que classifica o conteudo de lingua inglesa
data = PlainTextDocument(allDocuments, language="en")
# cria um ponteiro de função
strsplit_space_tokenizer <- function(x) unlist(strsplit(x, "[[:space:]]+"))
# vetor de argumentos para o metodo de processamento
ctrl <- list(tokenize = strsplit_space_tokenizer, # cada conjunto de letras separados por espaço sera considerada uma palavra
removePunctuation = list(preserve_intra_word_dashes = TRUE), # remove pontuação
stopwords = stopwords("en"),     # remove palavras insignificantes
stemming = TRUE,                 # passa as palavras para o radical
wordLengths = c(4, Inf))         # Retira palavras com menos de 4 letras
# Faz o processamento que retorna um vetor de frequancias de cada palavra presente em
# pelo menos um documento do dataset
frequency = termFreq(data, control = ctrl)
# lista de palavras presentes nos documentos
ret$vocabulary = names(frequency)
# lista de diretorios no diretorio raiz, que é usado como classes dos documentos
ret$classesDir = classesDir
# retorna o objeto que contem :
#         ret$numberOfDocuments : numero de documentos
#         ret$vocabulary        : palavras com ocorrencia
#         ret$classesDir        : classes de classificação
ret
}
computeProbability <- function(directory, vocabulary, numberOfDocuments) {
classesDir = system(paste("ls ",directory), intern=T)
allDocuments = c()
pwv_classes = NULL
pv = rep(0, length(classesDir))
for (i in 1:length(classesDir)) {
docList = system(paste("ls ",directory, "/", classesDir[i], sep=""), intern=T)
cat(i, " out of ", length(classesDir), "\n")
# computando probabilidade da classe p(v)
pv[i] = length(docList) / numberOfDocuments
joinDocs = c()
# percorrendo a classe
for (j in 1:length(docList)) {
document = paste(directory, "/", classesDir[i], "/", docList[j], sep="")
#PlainTextDocument(readLines(document))
joinDocs = c(joinDocs, readLines(document, encoding = "latin1"))
}
data = PlainTextDocument(joinDocs, language="en")
#data = removePunctuation(data)		# remove punctuation
#data = removeWords(data, stopwords())	# remove stopwords
#data = stripWhitespace(data)		# strip whitespaces
#data = removeNumbers(data)		# remove numbers
#data = stemDocument(data)		# steming
#frequency = termFreq(data)
strsplit_space_tokenizer <- function(x) unlist(strsplit(x, "[[:space:]]+"))
ctrl <- list(tokenize = strsplit_space_tokenizer,
removePunctuation = list(preserve_intra_word_dashes = TRUE),
stopwords = stopwords("en"),
stemming = TRUE,
wordLengths = c(4, Inf))
frequency = termFreq(data, control = ctrl)
n = length(frequency)
termsUnderThisClass = names(frequency)
counting = rep(0, length(vocabulary))
vocabIndex = c()
for (k in 1:length(termsUnderThisClass)) {
vocabIndex = c(vocabIndex, which(termsUnderThisClass[k] == vocabulary))
}
#print(vocabIndex)
counting[vocabIndex] = frequency
pwv_documentClass = (counting + 1) / (n + length(vocabulary))
pwv_classes = rbind(pwv_classes, pwv_documentClass)
}
ret = list()
ret$pv = pv
ret$pwv_classes = pwv_classes
ret
}
queryNaiveBayes <- function(naiveBayesModel, documentFilename) {
queryDoc = PlainTextDocument(readLines(documentFilename, encoding = "latin1"), language="en")
strsplit_space_tokenizer <- function(x) unlist(strsplit(x, "[[:space:]]+"))
ctrl <- list(tokenize = strsplit_space_tokenizer,
removePunctuation = list(preserve_intra_word_dashes = TRUE),
stopwords = stopwords("en"),
stemming = TRUE,
wordLengths = c(4, Inf))
frequency = termFreq(queryDoc, control = ctrl)
#print(naiveBayesModel$probs$pv)
result = naiveBayesModel$probs$pv #rep(0, length(naiveBayesModel$probs$pv))
#print(result)
termNames = names(frequency)
allProbs = naiveBayesModel$probs$pwv_classes
#allProbs = ((allProbs - min(allProbs)) / (max(allProbs) - min(allProbs))) + 1
for (j in 1:length(naiveBayesModel$probs$pv)) {
#result[j] = naiveBayesModel$probs$pv[j]
sum = 1
for (i in 1:length(termNames)) {
#prod = prod * naiveBayesModel$probs$pwv_classes[j,which(termNames[i] == naiveBayesModel$words$vocabulary)]
sum = sum + log(allProbs[j,which(termNames[i] == naiveBayesModel$words$vocabulary)])
#cat(termNames[i], " ", naiveBayesModel$probs$pwv_classes[j,which(termNames[i] == naiveBayesModel$words$vocabulary)], " ", prod, "\n")
}
result[j] = -log(result[j]) - sum
}
#print(result)
ret = list()
ret$relativeProbability = result / sum(result)
ret$classes = naiveBayesModel$words$classesDir
ret
}
naiveBayes <- function(directory) {
words = getAllWords(directory)
probs = computeProbability(directory, words$vocabulary, words$numberOfDocuments)
ret = list()
ret$words = words
ret$probs = probs
ret
}
test <- function(directory, model) {
classesDir = system(paste("ls ",directory), intern=T)
right = 0
wrong = 0
for (i in 1:length(classesDir)) {
docList = system(paste("ls ",directory, "/", classesDir[i], sep=""), intern=T)
cat(i, " out of ", length(classesDir), "\n")
for (j in 1:length(docList)) {
document = paste(directory, "/", classesDir[i], "/", docList[j], sep="")
result = queryNaiveBayes(model, document)
idx = which.min(result$relativeProbability)
#print(result$relativeProbability)
if (i == idx) {
right = right + 1
} else {
wrong = wrong + 1
}
cat("right: ", right, " wrong: ", wrong, "\n")
}
}
ret = list()
ret$right = right
ret$wrong = wrong
ret
}
model <- naiveBayes("Dropbox/Doutorado/disciplinas/SCC5871/trabalho/datasets/20_newsgroups3")
library(nlme)
library(lattice)
xyplot(weight ~ Time | Diet, BodyWeight)
?slom
??alom
??slom
??splom
install.packages("ggplot2")
install.packages("ggplot2")
install.packages("ggplot2")
library(ggplot2)
g <- ggplot(movies, aes(votes, rating))
print(g)
qplot(votes, rating, data = movies)
qplot(votes, rating, data = movies) + geom_smooth()
qplot(votes, rating, data = movies) + stats_smooth("loess")
qplot(votes, rating, data = movies)
qplot(votes, rating, data = movies) + geom_smooth()
qplot(votes, rating, data = movies)
qplot(votes, rating, data = movies, smooth = "loess")
qplot(votes, rating, data = movies, smooth = "loess")
qplot(votes, rating, data = movies, smooth = "less")
qplot(votes, rating, data = movies, panel = panel.loess)
qplot(votes, rating, data = movies) + geom_smooth()
?text
?lpoints
??lpoints
?trellis.par.set
??trellis.par.set
library(datasets)
data(airquality)
airquality = transform(airquality, Month = factor(Month))
qplot(Wind, Ozone, data = airquality, facets = . ~ Month)
qplot(Wind, Ozone, data = airquality, facets = . ~ factor(Month))
setwd("~/Dropbox/Doutorado/datasets/samsung")
f <- file(description="samsung.label",open="w")
for(j in 1:nrow(samsungData))
cat(samsungData[j,563],' ',file=f)
}
close(f)
f <- file(description="samsung.label",open="w")
for(j in 1:nrow(samsungData)){
cat(samsungData[j,563],' ',file=f)
}
close(f)
load("~/Dropbox/DataScience/Exploratory Data Analysis/data/samsungData.rda")
f <- file(description="samsung.label",open="w")
for(j in 1:nrow(samsungData)){
cat(samsungData[j,563],' ',file=f)
}
close(f)
f <- file(description="samsung.label",open="w")
for(j in 1:nrow(samsungData)){
cat(samsungData[j,563],file=f)
}
close(f)
f <- file(description="samsung.label",open="w")
for(j in 1:nrow(samsungData)){
cat(samsungData[j,563],'',file=f)
}
close(f)
